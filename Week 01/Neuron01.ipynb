{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPcerhETwa4D+BVvr6NYih5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"J0Ye6ZZS6mdp","executionInfo":{"status":"ok","timestamp":1686041865184,"user_tz":-360,"elapsed":5,"user":{"displayName":"Razeen Raiyun Kabir","userId":"15990856213194668306"}}},"outputs":[],"source":["import math\n","import numpy as np\n","\n","class Neuron:\n","    def __init__(self, num_inputs):\n","        self.weights = [.5] * num_inputs\n","        self.bias = 0\n","    \n","    def ReLU(self, x):\n","        if x<=0:\n","          return 0\n","        else:\n","          return x\n","\n","    \n","    def activate(self, inputs):\n","        \n","        # Calculate the weighted sum of inputs and bias\n","        weighted_sum = np.dot(inputs, self.weights) + self.bias\n","        print(weighted_sum)\n","        \n","        # Apply the activation function (in this case, sigmoid)\n","        activation = self.ReLU(weighted_sum)\n","        \n","        return activation"]},{"cell_type":"code","source":["neuron = Neuron(5)\n","inputs = [0.5, 0.7, -2, 5, -7]\n","output = neuron.activate(inputs)\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R1pFfQaE7RhH","executionInfo":{"status":"ok","timestamp":1686041934765,"user_tz":-360,"elapsed":9,"user":{"displayName":"Razeen Raiyun Kabir","userId":"15990856213194668306"}},"outputId":"a17efb90-7079-4479-f762-25f8f580cdfb"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["-1.4\n","0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Vw9wNwed7gZX"},"execution_count":null,"outputs":[]}]}